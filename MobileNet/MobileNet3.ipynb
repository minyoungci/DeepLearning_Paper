{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 쉽게 말해, 이 함수는 가까운 8의 배수를 찾아줌\n",
    "\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor) # divisor / 2 는 반올림을 위해 (너무 작아지지 않게)\n",
    "    # case 1) v=10, divisor = 8 이면 10+4 // 8 * 8 = 8 근데 10 => 8 은 10% 이상 빠지는 거니까 8+8 = 16 으로 조정됨\n",
    "    # case 2) v=39, divisor = 8 이면 39+4 // 8 * 8 = 40 => 10%보다 빠지지 않았기 때문에 40이 출력됨!\n",
    "\n",
    "    if new_v < 0.9 * v: # 10% 보다 더 빠지지 않게 조정\n",
    "        new_v += divisor\n",
    "\n",
    "    return new_v\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r = 4): # mobilenet V3 에서는 reduction ratio r=4로!\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.excitation = nn.Sequential(nn.Linear(in_channels, _make_divisible(in_channels // r, 8)),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(_make_divisible(in_channels // r, 8), in_channels),\n",
    "                                        nn.Hardsigmoid(inplace=True)) # Hard sigmoid!\n",
    "\n",
    "    def forward(self, x):\n",
    "        SE = self.squeeze(x)\n",
    "        SE = SE.reshape(x.shape[0],x.shape[1])\n",
    "        SE = self.excitation(SE)\n",
    "        SE = SE.unsqueeze(dim=2).unsqueeze(dim=3)\n",
    "        x = x * SE\n",
    "        return x\n",
    "\n",
    "class DepSESep(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, use_se, use_hs, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depthwise = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size, stride = stride, padding = (kernel_size - 1) // 2, groups = in_channels, bias=False),\n",
    "                                       nn.BatchNorm2d(in_channels, momentum=0.99), # momentum = 0.99 는 논문에서 제시\n",
    "                                       nn.Hardswish(inplace=True) if use_hs else nn.ReLU(inplace=True)) # hs 아니면 걍 ReLU (ReLU6 아님)\n",
    "\n",
    "        self.seblock = SEBlock(in_channels) if use_se else None\n",
    "\n",
    "        self.pointwise = nn.Sequential(nn.Conv2d(in_channels, out_channels,1, bias=False),\n",
    "                                       nn.BatchNorm2d(out_channels, momentum=0.99))\n",
    "                                       # no activation!!\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        if self.seblock is not None:\n",
    "            x = self.seblock(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class InvertedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, exp_channels, out_channels, kernel_size, stride, use_se, use_hs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_skip_connect = (stride==1 and in_channels==out_channels)\n",
    "\n",
    "        layers = []\n",
    "        if in_channels != exp_channels: # 채널 안늘어날 때는 1x1 생략. 즉, 1x1은 채널을 키워야할 때만 존재한다.\n",
    "            layers += [nn.Sequential(nn.Conv2d(in_channels, exp_channels, 1, bias=False),\n",
    "                                     nn.BatchNorm2d(exp_channels, momentum=0.99),\n",
    "                                     nn.Hardswish(inplace=True) if use_hs else nn.ReLU(inplace=True))]\n",
    "        layers += [DepSESep(exp_channels, out_channels, kernel_size, use_se, use_hs, stride=stride)]\n",
    "\n",
    "        self.residual = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_skip_connect:\n",
    "            return x + self.residual(x) # 더하고 ReLU 하지 않는다! 그래야 linear block이 되는 거니까\n",
    "        else:\n",
    "            return self.residual(x)\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, last_channels, num_classes=1000, width_mult=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        in_channels = _make_divisible(16 * width_mult, 8)\n",
    "\n",
    "        # building first layer\n",
    "        self.stem_conv = nn.Sequential(nn.Conv2d(3, in_channels, 3, padding=1, stride=2, bias=False),\n",
    "                                       nn.BatchNorm2d(in_channels, momentum=0.99),\n",
    "                                       nn.Hardswish(inplace=True)) # 처음건 무조건 HS, HS를 써서 16으로 줄일 수 있었다 함\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        layers=[]\n",
    "        for k, t, c, use_se, use_hs, s in cfgs:\n",
    "            exp_channels = _make_divisible(in_channels * t, 8)\n",
    "            out_channels = _make_divisible(c * width_mult, 8)\n",
    "            layers += [InvertedBlock(in_channels, exp_channels, out_channels, k, s, use_se, use_hs)]\n",
    "            in_channels = out_channels\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # building last several layers\n",
    "        self.last_conv = nn.Sequential(nn.Conv2d(in_channels, exp_channels, 1, bias=False),\n",
    "                                       nn.BatchNorm2d(exp_channels, momentum=0.99),\n",
    "                                       nn.Hardswish(inplace=True))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        last_channels = _make_divisible(last_channels * width_mult, 8)\n",
    "        self.classifier = nn.Sequential(nn.Linear(exp_channels, last_channels),\n",
    "                                        nn.Hardswish(inplace=True),\n",
    "                                        nn.Dropout(p=0.2, inplace=True),\n",
    "                                        nn.Linear(last_channels, num_classes)) # MLP 부활\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight) # nn.init.constant_(m.weight, 1) 말고 이런 방법도 있음\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def mobilenetv3_large(**kwargs):\n",
    "    cfgs = [#k,   t,   c,   SE,   HS,   s\n",
    "            # 이전 output에 t를 곱해서 exp size가 되는 것임!\n",
    "            [3,   1,  16, False, False, 1],\n",
    "            [3,   4,  24, False, False, 2],\n",
    "            [3,   3,  24, False, False, 1],\n",
    "            [5,   3,  40, True,  False, 2],\n",
    "            [5,   3,  40, True,  False, 1],\n",
    "            [5,   3,  40, True,  False, 1],\n",
    "            [3,   6,  80, False, True,  2],\n",
    "            [3, 2.5,  80, False, True,  1],\n",
    "            [3, 2.3,  80, False, True,  1],\n",
    "            [3, 2.3,  80, False, True,  1],\n",
    "            [3,   6, 112, True,  True,  1],\n",
    "            [3,   6, 112, True,  True,  1],\n",
    "            [5,   6, 160, True,  True,  2],\n",
    "            [5,   6, 160, True,  True,  1],\n",
    "            [5,   6, 160, True,  True,  1]]\n",
    "\n",
    "    return MobileNetV3(cfgs, last_channels=1280, **kwargs)\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    cfgs = [#k,    t,   c,  SE,    HS,   s\n",
    "            [3,    1,  16, True,  False, 2],\n",
    "            [3,  4.5,  24, False, False, 2],\n",
    "            [3, 3.67,  24, False, False, 1],\n",
    "            [5,    4,  40, True,  True,  2],\n",
    "            [5,    6,  40, True,  True,  1],\n",
    "            [5,    6,  40, True,  True,  1],\n",
    "            [5,    3,  48, True,  True,  1],\n",
    "            [5,    3,  48, True,  True,  1],\n",
    "            [5,    6,  96, True,  True,  2],\n",
    "            [5,    6,  96, True,  True,  1],\n",
    "            [5,    6,  96, True,  True,  1]]\n",
    "\n",
    "    return MobileNetV3(cfgs, last_channels=1024, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "MobileNetV3                                             [2, 1000]                 --\n",
       "├─Sequential: 1-1                                       [2, 16, 112, 112]         --\n",
       "│    └─Conv2d: 2-1                                      [2, 16, 112, 112]         432\n",
       "│    └─BatchNorm2d: 2-2                                 [2, 16, 112, 112]         32\n",
       "│    └─Hardswish: 2-3                                   [2, 16, 112, 112]         --\n",
       "├─Sequential: 1-2                                       [2, 160, 7, 7]            --\n",
       "│    └─InvertedBlock: 2-4                               [2, 16, 112, 112]         --\n",
       "│    │    └─Sequential: 3-1                             [2, 16, 112, 112]         464\n",
       "│    └─InvertedBlock: 2-5                               [2, 24, 56, 56]           --\n",
       "│    │    └─Sequential: 3-2                             [2, 24, 56, 56]           3,440\n",
       "│    └─InvertedBlock: 2-6                               [2, 24, 56, 56]           --\n",
       "│    │    └─Sequential: 3-3                             [2, 24, 56, 56]           4,440\n",
       "│    └─InvertedBlock: 2-7                               [2, 40, 28, 28]           --\n",
       "│    │    └─Sequential: 3-4                             [2, 40, 28, 28]           10,328\n",
       "│    └─InvertedBlock: 2-8                               [2, 40, 28, 28]           --\n",
       "│    │    └─Sequential: 3-5                             [2, 40, 28, 28]           20,992\n",
       "│    └─InvertedBlock: 2-9                               [2, 40, 28, 28]           --\n",
       "│    │    └─Sequential: 3-6                             [2, 40, 28, 28]           20,992\n",
       "│    └─InvertedBlock: 2-10                              [2, 80, 14, 14]           --\n",
       "│    │    └─Sequential: 3-7                             [2, 80, 14, 14]           32,080\n",
       "│    └─InvertedBlock: 2-11                              [2, 80, 14, 14]           --\n",
       "│    │    └─Sequential: 3-8                             [2, 80, 14, 14]           34,760\n",
       "│    └─InvertedBlock: 2-12                              [2, 80, 14, 14]           --\n",
       "│    │    └─Sequential: 3-9                             [2, 80, 14, 14]           31,992\n",
       "│    └─InvertedBlock: 2-13                              [2, 80, 14, 14]           --\n",
       "│    │    └─Sequential: 3-10                            [2, 80, 14, 14]           31,992\n",
       "│    └─InvertedBlock: 2-14                              [2, 112, 14, 14]          --\n",
       "│    │    └─Sequential: 3-11                            [2, 112, 14, 14]          214,424\n",
       "│    └─InvertedBlock: 2-15                              [2, 112, 14, 14]          --\n",
       "│    │    └─Sequential: 3-12                            [2, 112, 14, 14]          386,120\n",
       "│    └─InvertedBlock: 2-16                              [2, 160, 7, 7]            --\n",
       "│    │    └─Sequential: 3-13                            [2, 160, 7, 7]            429,224\n",
       "│    └─InvertedBlock: 2-17                              [2, 160, 7, 7]            --\n",
       "│    │    └─Sequential: 3-14                            [2, 160, 7, 7]            797,360\n",
       "│    └─InvertedBlock: 2-18                              [2, 160, 7, 7]            --\n",
       "│    │    └─Sequential: 3-15                            [2, 160, 7, 7]            797,360\n",
       "├─Sequential: 1-3                                       [2, 960, 7, 7]            --\n",
       "│    └─Conv2d: 2-19                                     [2, 960, 7, 7]            153,600\n",
       "│    └─BatchNorm2d: 2-20                                [2, 960, 7, 7]            1,920\n",
       "│    └─Hardswish: 2-21                                  [2, 960, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-4                                [2, 960, 1, 1]            --\n",
       "├─Sequential: 1-5                                       [2, 1000]                 --\n",
       "│    └─Linear: 2-22                                     [2, 1280]                 1,230,080\n",
       "│    └─Hardswish: 2-23                                  [2, 1280]                 --\n",
       "│    └─Dropout: 2-24                                    [2, 1280]                 --\n",
       "│    └─Linear: 2-25                                     [2, 1000]                 1,281,000\n",
       "=========================================================================================================\n",
       "Total params: 5,483,032\n",
       "Trainable params: 5,483,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 433.24\n",
       "=========================================================================================================\n",
       "Input size (MB): 1.20\n",
       "Forward/backward pass size (MB): 140.91\n",
       "Params size (MB): 21.93\n",
       "Estimated Total Size (MB): 164.05\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mobilenetv3_large()\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(2,3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,224,224)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
