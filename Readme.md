# Deep learning Paper Review

* 최신 논문 및 관심있는 논문을 리뷰하고 구현합니다.


### Computer Vision 

* Yolo v1
  * [욜로! 혁명의 시작](https://minyoungxi.tistory.com/53)
  * Paper : [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) / [code](https://velog.io/@minyoungxi/YOLO-v1-%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%97%90%EC%84%9C-%EA%B0%9D%EC%B2%B4-%ED%83%90%EC%A7%80%EB%A5%BC-%ED%95%B4%EB%B3%BC%EA%B9%8C-%EB%85%BC%EB%AC%B8-%EA%B5%AC%ED%98%84-part1.-model)
 
* Yolo v2
  * [조금 더 개선된 yolo ?!](https://velog.io/@minyoungxi/yolo-v2-%EC%A1%B0%EA%B8%88-%EB%8D%94-%EA%B0%9C%EC%84%A0%EB%90%9C-yolo-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
  * [v1에서 v2로 진화 !!](https://velog.io/@minyoungxi/yolov2-v1%EC%97%90%EC%84%9C-v2%EB%A1%9C-%EC%A7%84%ED%99%94-%EB%AA%A8%EB%8D%B8-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
  * Paper : [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242)
 
* Yolo v3
  * [뭐? abox가 10000개라고?](https://velog.io/@minyoungxi/YOLOv3-%EB%AD%90-abox%EA%B0%80-10000%EA%B0%9C%EB%9D%BC%EA%B3%A0-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
  * Paper : YOLOv3: [An Incremental Improvement](https://arxiv.org/abs/1804.02767)

* Yolo v4
  * [이제부터 진짜다 !](https://velog.io/@minyoungxi/YOLO-v4-%EC%9D%B4%EC%A0%9C%EB%B6%80%ED%84%B0-%EC%A7%84%EC%A7%9C%EB%8B%A4-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-part1)
  * [성능을 향상시켜보자!](https://velog.io/@minyoungxi/YOLO-v4-%EC%84%B1%EB%8A%A5%EC%9D%84-%ED%96%A5%EC%83%81%EC%8B%9C%EC%BC%9C%EB%B3%B4%EC%9E%90-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-part2)
  * Paper : YOLOv4: [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)
 
* SAM
  * [싹 다 segmentation 해버려! ](https://minyoungxi.tistory.com/91)
  * Paper : [Segment Anything](https://arxiv.org/abs/2304.02643)

* MobileNet v1
  * [Mobilenetv1](https://publish.obsidian.md/minyoungxi/Paper/Basic/Mobilenet/MobileNets+-+Efficient+Convolutional+Neural+Networks+for+Mobile+Vision+Applications)
 
* MobileNet v2
  * [Mobilenetv2](https://publish.obsidian.md/minyoungxi/Paper/Basic/Mobilenet/MobileNetV2+-+Inverted+Residuals+and+Linear+Bottlenecks)

* MobileNet v3
  * [Mobilenetv3](https://publish.obsidian.md/minyoungxi/Paper/Basic/Mobilenet/MobileNetV3)
 
* ResNet
  * [ResNet](https://publish.obsidian.md/minyoungxi/Paper/Basic/ResNet/ResNet+-+Deep+Residual+Learning+for+Image+Recognition)
  * [WideResNet](https://publish.obsidian.md/minyoungxi/Paper/Basic/ResNet/Wide+Residual+Networks)
  * [Aggregated Residual Transformations for Deep Neural Networks](https://publish.obsidian.md/minyoungxi/Paper/Basic/ResNet/Aggregated+Residual+Transformations+for+Deep+Neural+Networks)
 

### Transformer

* Vision Transformer
  * [어텐션을 컴퓨터 비전에? - part1](https://minyoungxi.tistory.com/51)
  * [어텐션을 컴퓨터 비전에? - part2](https://minyoungxi.tistory.com/52)
  * Paper : [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
 
* Efficient ViT
  * [EfficientViT](https://publish.obsidian.md/minyoungxi/Paper/Transformer/Vision+Transformer/EfficientViT+-+Memory+Efficient+Vision+Transformer+with+Cascaded+Group+Attention)

* Swin Transformer
  * [ViT를 개선해보자](https://minyoungxi.tistory.com/56)
  * [Swin Transformer 리뷰](https://publish.obsidian.md/minyoungxi/Paper/Transformer/Swin+transformer+-+Hierarchical+vision+transformer+using+shifted+windows)
  * Paper : [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
 
* Survey - Sparse Attention
   * [Sparse한 attention](https://publish.obsidian.md/minyoungxi/Paper/Transformer/survey/Sparse+Attention)
 
* [BERT - 파인튜닝이 젤 중요해!!](https://publish.obsidian.md/minyoungxi/Paper/Transformer/NLP/BERT+-+Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding) 

### eXplainable Artificial Intelligence

* LIME
  * [“Why Should I Trust You?”](https://velog.io/@minyoungxi/LIME-Why-Should-I-Trust-You)
  * Paper : [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)

* CAM
  * [어이! 너 어딜 보는거야?](https://minyoungxi.tistory.com/55)
  * Paper : [Learning Deep Features for Discriminative Localization](https://arxiv.org/pdf/1512.04150.pdf)

### Natural Language Processing 

* Attention is all you need
  * [어텐션 플리즈](https://minyoungxi.tistory.com/71)
  * Paper : [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  * code : [Transformer](https://github.com/minyoungci/DeepLearning_Paper/tree/master/attention%20is%20all%20you%20need)



### Generative 
* diffusion model
  *[Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://minyoungxi.tistory.com/64)
  * Paper : [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)

* StyleGAN 2019
  * [styleGAN 2019 - 고해상도 이미지를 고퀄로 생성해보자](https://minyoungxi.tistory.com/42)
  * Paper : [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/pdf/1912.04958.pdf)
 
* StyleGAN2 ( CVPR2020 )
  * [styleGAN2(CVPR2020) - StyleGAN의 단점ㅇ을 보완해보자](https://minyoungxi.tistory.com/43)
  * Paper : [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/pdf/1912.04958.pdf)

### Medical 

* [Transformers in Medical Image Analysis - A Review](https://publish.obsidian.md/minyoungxi/Paper/medical/Transformers+in+Medical+Image+Analysis+-+A+Review)

* [Swin UNETR](https://publish.obsidian.md/minyoungxi/Paper/medical/segmentation/Swin+UNETR+-+Swin+Transformers+for+Semantic+Segmentation+of+Brain+Tumors+in+MRI+Images)

* [Towards Expert-Level Medical Question Answering with Large Language Models](https://minyoungxi.tistory.com/98)
     * [Med-PaLM2](https://publish.obsidian.md/minyoungxi/Paper/medical/LLM/Towards+Expert-Level+Medical+Question+Answering+with+Large+Language+Models)
* [Large Language Models Encode Clinical Knowledge](https://minyoungxi.tistory.com/97)
     * [Med-PaLm](https://publish.obsidian.md/minyoungxi/Paper/medical/LLM/Large+Language+Models+Encode+Clinical+Knowledge)


### Self-Supervised Learning

* [A simple framework for contrastive learning of visual representations (SimCLR)](https://minyoungxi.tistory.com/82)
